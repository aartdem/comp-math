# Отчет по задаче 1
Все измерения проводились на следующем оборудовании:
* Процессор - 11th Gen Intel® Core™ i5-11500 @ 2.70GHz (12 ядер);
* Оперативная память - 16ГБ;
* Операционная система - Ubuntu 22.04.4 LTS;
* Версия gcc - 11.4.0.

Под временем работы алгоритма понимается время работы метода `calculate_approximation()`, таким образом не учитывается время, которое тратися на выделение памяти и инициализацию функции начальным приближением.

Для вычисления ошибки аппроксимации использовалась формула:
$$e=\dfrac 1 {N^2} \displaystyle\sum_{i = 1}^N\sum_{j = 1}^N \dfrac {\lvert u(i,j)- g(ih, jh)\rvert} {\lvert g(ih,jh)\rvert}, $$    
где $N$ - количество неграничных узлов в сетке, $h =\frac {1} {N+1}$, $g$ - функция, которую мы приближаем, $u(i,j)$ - значение получившейся функции в узле $(i,j)$. Для визуализации результатов в виде графиков использовался python версии 3.10.12 и библиотеки, описанные в [requirements.txt](https://github.com/aartdem/comp-math/blob/main/work1/requirements.txt)

```g++ main.cpp -fopenmp -o main.out && ./main.out && python3 graph.py```
# Начальное приближение
Чтобы понять, какое выбрать начальное приближение, было проведено несколько измерений. Все измерения проводились для $eps=0.001$, с размером блока = 32 и четрырех потоках. Основная цель - понять, зависит ли время работы алгоритма от начального приближения, и если зависит, то как оптимально подобрать начальное приближение для данной функции. Исследовалось 3 способа инициализации начального приближения для каждой клетки:
* ноль;
* среднее множества $G$;
* случайное значение в диапазоне от $min(G)$ до $max(G)$
  где $G$ - множество значений функции в граничных точках.

Итак, было проведено четыре эксперимента, результаты отражены на графиках:
